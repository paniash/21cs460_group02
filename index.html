<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="https://www.niser.ac.in/~smishra/teach/cs460/2021/project/21cs460_group02/libs/katex/katex.min.css">


  <link rel="stylesheet" href="https://www.niser.ac.in/~smishra/teach/cs460/2021/project/21cs460_group02/css/franklin.css">
   <title>Investigating ferromagnetic transitions using ML &#40;Final Presentation&#41;</title>
</head>
<body>

<!-- Content appended here -->
<div class="franklin-content"><h1 id="investigating_ferromagnetic_transitions_using_ml_final_presentation"><a href="#investigating_ferromagnetic_transitions_using_ml_final_presentation" class="header-anchor">Investigating ferromagnetic transitions using ML &#40;Final Presentation&#41;</a></h1>
<p>Team members: Ashish Panigrahi and S. Gautameshwar.</p>
<p>For the midway progress of the project, see <a href="https://www.niser.ac.in/~smishra/teach/cs460/2021/project/21cs460_group02/midway/index.html">here</a>. All the code used for this project is hosted on GitHub at <a href="https://github.com/paniash/isingModel">this link</a>. The slides for the presentation can be found <a href="https://github.com/paniash/21cs460_group02/raw/main/assets/final-pres.pdf">here</a>.</p>
<h2 id="overview_of_the_ising_model"><a href="#overview_of_the_ising_model" class="header-anchor">Overview of the Ising Model</a></h2>
<p>Previously covered during the midway presentation, we deal briefly with the one-dimensional Ising model and in the second half of the project, present the two-dimensional Ising spin model consisting of either spin-\(\uparrow\) or spin-\(\downarrow\).</p>

<figure style="text-align:center;">
<img src="https://github.com/paniash/21cs460_group02/raw/main/assets/ising-model.png" style="padding:0; width:20em;" alt=" Fig 1: 2D lattice consisting of spin-1/2 particles"/>
<figcaption> Fig 1: 2D lattice consisting of spin-1/2 particles</figcaption>
</figure>

<p>The Hamiltonian of the Ising model is given by</p>
\[
    H(\sigma) = - \sum_{\langle i, j \rangle} J_{ij} \sigma_i \sigma_j
\]
<p>where the summation is carried over the first nearest neighbours i.e. \(j = i+1\) or \(j = i-1\). Hence the equivalent total &quot;eigenenergy&quot; of the lattice is similarly given by</p>
\[
    E(\sigma) = - \sum_{\langle i, j \rangle} J_{ij} \sigma_i \sigma_j
\]
<p>here \(\sigma_i = \pm1\), giving a scalar value for energy.</p>
<h2 id="pre-midway_work"><a href="#pre-midway_work" class="header-anchor">Pre-midway work</a></h2>
<p>We investigated regression models for the one-dimensional Ising spin model consisting of 50 spin-1/2 particles. The idea was to be able to find the parameter quantifying interaction between the spin particles i.e. the coupling constant \(J\).</p>
<p>The assumption made was that the interaction is limited to only the first nearest neighbours of a spin particle &#40;depicted by the notation \(\langle i, j \rangle\) under the summation in the equation for the Hamiltonian&#41;.</p>
<p>We trained our model using linear regression and its variants &#40;Ridge and Lasso regression&#41;. Comparison between the various models was observed with linear regression giving a bad model due to overfitting. In contrast, Ridge and Lasso eliminated the problem of overfitting and gave us a fairly performant model.</p>
<p>To extend our model, we also experimented with the case where instead of a first-neighbour, extension of the interaction upto the second nearest neighbour was assumed and a model was trained on it using Ridge regression which gave us pretty good results.</p>
<h2 id="premise_to_investigating_the_2d_ising_model"><a href="#premise_to_investigating_the_2d_ising_model" class="header-anchor">Premise to investigating the 2D Ising model</a></h2>

<figure style="text-align:center;">
<img src="https://github.com/paniash/21cs460_group02/raw/main/assets/images/phase-types.png" style="padding:0; width:40em;" alt=" "/>
<figcaption> </figcaption>
</figure>

<p>The idea for post-midway work involving the two-dimensional Ising model is investigating 2D lattices made with spin-1/2 particles with the dynamics of the orientation of these particles governed by the temperature of the lattice where there exists a critical temperature \(T_c\) beyond which an originally ferromagnetic material becomes paramagnetic.</p>
<h2 id="analysis_of_the_monte-carlo_data_for_the_model"><a href="#analysis_of_the_monte-carlo_data_for_the_model" class="header-anchor">Analysis of the Monte-Carlo data for the model</a></h2>
<ul>
<li><p>For the testing and training data for our model, we generate our data using Monte-Carlo methods i.e. the metropolis algorithm.</p>
</li>
<li><p>We took a temperature range of &#40;0.25-4.0&#41; units <sup id="fnref:2"><a href="#fndef:2" class="fnref">[1]</a></sup> at an interval of 0.25 units and generated 10,000 lattices for every temperature for a total of 200,000 lattices.</p>
</li>
<li><p>Corresponding plots of energy and total heat capacity as a function of temperature are made for verifying the reliability of the Monte-Carlo data in accordance with statistical mechanics.</p>
</li>
</ul>
<table class="fndef" id="fndef:2">
    <tr>
        <td class="fndef-backref"><a href="#fnref:2">[1]</a></td>
        <td class="fndef-content">The unit for temperature is the natural units where we assume \(h = 1\), \(c = 1\) and so on.</td>
    </tr>
</table>

<h2 id="labelling_data_before_supervised_learning"><a href="#labelling_data_before_supervised_learning" class="header-anchor">Labelling data before supervised learning</a></h2>
<p>While generating our data, we use a coupling strength value of \(J=2\). How does one proceed to determine the critical temperature? Theoretically shown by Onsager <em>et al.</em>, \(T_c\) is given by</p>
\[
T_c = J/ \log (1 + \sqrt{2}) \approx 2.26
\]
<p>Afterwards, we split this data into critical and ordered/disordered sets and we expect the model to struggle in identifying the type of orderness of the lattices in the vicinity of the critical temperature.</p>
<h2 id="vicinity_of_critical_temperature"><a href="#vicinity_of_critical_temperature" class="header-anchor">Vicinity of critical temperature</a></h2>

<figure style="text-align:center;">
<img src="https://github.com/paniash/21cs460_group02/raw/main/assets/images/critical-region.png" style="padding:0; width:40em;" alt=" "/>
<figcaption> </figcaption>
</figure>

<p>In the left plot, the region marked red refers to the vicinity of the critical temperature where we label our lattices to be in the critical phase. The whole premise is for the model, given a lattice configuration, be able to determine if the phase is ordered or disordered.</p>
<h2 id="random_forests_the_way_to_go"><a href="#random_forests_the_way_to_go" class="header-anchor">Random Forests: the way to go?</a></h2>
<ul>
<li><p>Since our data was Monte-Carlo generated, we expect our predictions to be correlated. Weak classifiers such as decision trees being algorithms with high variance are not a good choice for such correlated data but introducing ensemble learning techniques involving bagging and averaging of these decision trees helps lower the overall bias and variance, the algorithm which we commonly know as Random Forests &#91;3&#93;.</p>
</li>
<li><p>This helps generate a better performing model.</p>
</li>
</ul>
<h2 id="implementation_of_random_forests"><a href="#implementation_of_random_forests" class="header-anchor">Implementation of Random Forests</a></h2>
<p>We used <a href="https://scikit-learn.org/stable/index.html">scikit-learn</a> to implement our algorithm. The training was done on our ordered and disordered lattice datasets using the RF classifier scheme. However, the critical lattice samples were <strong>not</strong> used for training since we use it as a certificate for verifying the reliability of our model when classifying between the ordered and disordered phases especially within the critical regime &#40;region we earlier marked red&#41;.</p>
<p>In the RF algorithm, two hyperparameters play significant roles in determining if our model works fairly well:     - <code>n_estimators</code> i.e. the number of decision trees in our forest.     - <code>min_samples_split</code> i.e. the leaf size &#40;number of features&#41; at each node.</p>
<p>During the implementation, we compared course trees &#40;<code>min_samples_split</code> &#61; 10000&#41; with fine trees &#40;<code>min_samples_split</code> &#61; 2&#41;.</p>
<h2 id="results_from_mehta_et_al"><a href="#results_from_mehta_et_al" class="header-anchor">Results from Mehta <em>et al.</em></a></h2>

<figure style="text-align:center;">
<img src="https://github.com/paniash/21cs460_group02/raw/main/assets/images/mehta.png" style="padding:0; width:40em;" alt=" "/>
<figcaption> </figcaption>
</figure>

<p>Mehta <em>et al.</em> &#91;1&#93; implemented the random forests algorithm on Monte-Carlo generated data and as seen from the above plot, we see a training and testing score of almost 100&#37;, which is expected since both the training and testing data were generated using the same Monte-Carlo approach.</p>
<p>However, the catch comes up when seeing the scores for critical samples. Accuracy scores of 69.2&#37; and 83.1&#37; were seen for course and fine trees respectively at 100 estimators each.</p>
<p>Is there a way we can improve on this score?</p>
<h2 id="transductive_approach_-_an_improvement_to_vanilla_rf"><a href="#transductive_approach_-_an_improvement_to_vanilla_rf" class="header-anchor">Transductive approach - an improvement to vanilla RF</a></h2>
<ul>
<li><p>In our original model implemented using standard RF algorithm, all lattices regardless of the state of order are given equal importance &#40;weightage&#41; based on which the hierarchy of features in the decision trees is determined.</p>
</li>
<li><p>Since we know &#40;and also saw in the results by Mehta <em>et al.</em>&#41; that the model struggles in the critical regime, our approach involves giving more weightage to lattice datapoints closer to the critical temperature over the ones that are farther away from it. This allows the model to pick more prominent features in these lattices.</p>
</li>
<li><p>It also lowers the priority of features present in the extreme ends of the orderness i.e. completely ordered/disordered since they don&#39;t play as important of a role as the lattices that are present in the neighbourhood of the critical point.</p>
</li>
</ul>
<h2 id="implementation_of_this_weighted_approach"><a href="#implementation_of_this_weighted_approach" class="header-anchor">Implementation of this weighted approach</a></h2>
<p>We simply cloned &#40;duplicated&#41; the lattice Ising data for points that were closer to the critical temperature \(T_c\) in order to increase the weights for these lattices compared to the farther ones for which no further cloning was done.</p>

<figure style="text-align:center;">
<img src="https://github.com/paniash/21cs460_group02/raw/main/assets/images/region.png" style="padding:0; width:40em;" alt=" More weights given to points in the red region compared to its neighbours who have more weights than their extremities."/>
<figcaption> More weights given to points in the red region compared to its neighbours who have more weights than their extremities.</figcaption>
</figure>

<h2 id="results"><a href="#results" class="header-anchor">Results</a></h2>

<figure style="text-align:center;">
<img src="https://github.com/paniash/21cs460_group02/raw/main/assets/images/training.png" style="padding:0; width:30em;" alt=" Training scores obtained with our approach"/>
<figcaption> Training scores obtained with our approach</figcaption>
</figure>


<figure style="text-align:center;">
<img src="https://github.com/paniash/21cs460_group02/raw/main/assets/images/testing.png" style="padding:0; width:30em;" alt=" Testing scores obtained with our approach"/>
<figcaption> Testing scores obtained with our approach</figcaption>
</figure>


<figure style="text-align:center;">
<img src="https://github.com/paniash/21cs460_group02/raw/main/assets/images/critical-score.png" style="padding:0; width:30em;" alt=" Critical scores"/>
<figcaption> Critical scores</figcaption>
</figure>

<p>The results we obtained with our weighted approach shows an increase in the accuracy of our model when classifying critical samples compared to vanilla RF. Interestingly, by emphasising on lattice samples closer to the critical temperature \(T_c\), our approach works better for coarse trees with an accuracy of 92.0&#37; compared to fine ones &#40;88.75&#37;&#41;.</p>
<p>Overall, we see a huge improvement over a vanilla RF critical score of 69.2&#37;.</p>
<h2 id="conclusion"><a href="#conclusion" class="header-anchor">Conclusion</a></h2>
<ul>
<li><p>Our modified random forests algorithm is effective especially in classifying lattices in the critical region &#40;along with other regions&#41;. Unfortunately, a downside to this algorithm is that it is computationally expensive with a runtime of ~90-170 seconds.</p>
</li>
<li><p>The accuracy of our model is comparable with unsupervised methods such as convolutional neural networks &#40;CNNs&#41; with an accuracy of ~88-92&#37; &#91;1&#93;.</p>
</li>
</ul>
<h2 id="limitations_and_future_work"><a href="#limitations_and_future_work" class="header-anchor">Limitations and future work</a></h2>
<ul>
<li><p>We however, need good analytical data for our material in order to implement this algorithm for it demands us to know the critical temperature of the material beforehand &#40;since we are using a supervised approach&#41; with labels being assigned manually for ordered/disordered lattices to get a well-trained model.</p>
</li>
<li><p>We hope to work using an unsupervised approach as a next step in using a &quot;generative&quot; as opposed to a &quot;discriminative&quot; model in the future. Such models are capable of generating data for a new critical phase lattice using information from lattices used in our training data. This is achieved using restricted Boltzmann machines and deep Boltzmann machines &#91;2&#93;.</p>
</li>
</ul>
<h2 id="bibliography"><a href="#bibliography" class="header-anchor">Bibliography</a></h2>
<ol>
<li><p>Pankaj Mehta et al. “A high-bias, low-variance introduction to machine learning for physicists”. In: Physics reports 810 &#40;2019&#41;, pp. 1-124</p>
</li>
<li><p>Morningstar, Alan, and Roger G. Melko. &quot;Deep learning the ising model near criticality.&quot; arXiv preprint arXiv:1708.04622 &#40;2017&#41;.</p>
</li>
<li><p>Breiman, Leo. &quot;Random forests.&quot; Machine learning 45.1 &#40;2001&#41;: 5-32.</p>
</li>
<li><p>Deisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. Mathematics for machine learning. Cambridge University Press, 2020.</p>
</li>
</ol>
<div class="page-foot">
  <div class="copyright">
  </div>
</div>
</div><!-- CONTENT ENDS HERE -->

        <script src="https://www.niser.ac.in/~smishra/teach/cs460/2021/project/21cs460_group02/libs/katex/katex.min.js"></script>
<script src="https://www.niser.ac.in/~smishra/teach/cs460/2021/project/21cs460_group02/libs/katex/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>



  </body>
</html>
